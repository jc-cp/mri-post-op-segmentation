{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import dataclasses\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main IDs extraction and reorganization of BraTS23 Adult Glioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patient_scan_id(folder_name):\n",
    "    parts = folder_name.split('-')\n",
    "    if len(parts) >= 4:\n",
    "        return str(parts[2]), str(parts[3])\n",
    "    return None, None\n",
    "\n",
    "def extract_sequence(file_name):\n",
    "    parts = file_name.split('-')\n",
    "    if len(parts) > 1:\n",
    "        return parts[-1].split('.')[0]\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = Path(\"/mnt/93E8-0534/JuanCarlos/\")\n",
    "assert target_path.exists()\n",
    "dataset_name = \"BraTS-GLI-2024\"\n",
    "dataset_path = target_path / dataset_name\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "input_path = dataset_path / \"training_data\"\n",
    "os.makedirs(input_path, exist_ok=True)\n",
    "output_path = dataset_path / \"organized_dataset\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Create new folders for images and labels\n",
    "images_out = output_path / \"images\"\n",
    "labels_out = output_path / \"labels\"\n",
    "os.makedirs(images_out, exist_ok=True)\n",
    "os.makedirs(labels_out, exist_ok=True)\n",
    "\n",
    "train_images_out = images_out / \"training\"\n",
    "val_images_out = images_out / \"validation\"\n",
    "test_images_out = images_out / \"testing\"\n",
    "train_labels_out = labels_out / \"training\"\n",
    "val_labels_out = labels_out / \"validation\"\n",
    "test_labels_out = labels_out / \"testing\"\n",
    "\n",
    "for folder in [train_images_out, val_images_out, test_images_out, \n",
    "               train_labels_out, val_labels_out, test_labels_out]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "output_file = output_path / \"dataset_info.csv\"\n",
    "training_csv_file = output_path / \"training_dataset_info.csv\"\n",
    "validation_csv_file = output_path / \"validation_dataset_info.csv\"\n",
    "test_csv_file = output_path / \"testing_dataset_info.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dont forget to load the training and validation data from Brats into the data folder, unzip it and rename the folders to training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patients: 613\n",
      "Training patients: 490\n",
      "Validation patients: 61\n",
      "Testing patients: 62\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(input_path, output_path, main_csv_file, training_csv_file, validation_csv_file, testing_csv_file, dataset_name):\n",
    "    sequence_mapping = {'seg':0, 't1n': 1, 't1c': 2, 't2w': 3, 't2f': 4}\n",
    "   \n",
    "    with open(main_csv_file, 'w', newline='') as main_csvfile, \\\n",
    "         open(training_csv_file, 'w', newline='') as train_csvfile, \\\n",
    "         open(validation_csv_file, 'w', newline='') as val_csvfile, \\\n",
    "         open(testing_csv_file, 'w', newline='') as test_csvfile:\n",
    "       \n",
    "        main_writer = csv.writer(main_csvfile)\n",
    "        train_writer = csv.writer(train_csvfile)\n",
    "        val_writer = csv.writer(val_csvfile)\n",
    "        test_writer = csv.writer(test_csvfile)\n",
    "       \n",
    "        headers = ['PatientID', 'ScanID', 'Sequence', 'SequenceLabel', 'Dataset', 'HasSegmentation']\n",
    "        for writer in [main_writer, train_writer, val_writer, test_writer]:\n",
    "            writer.writerow(headers)\n",
    "       \n",
    "        dataset = dataset_name\n",
    "       \n",
    "        # Process the training set\n",
    "        patient_data = defaultdict(list)\n",
    "        train_input_path = input_path \n",
    "        for patient_folder in train_input_path.iterdir():\n",
    "            if patient_folder.is_dir():\n",
    "                patient_id, scan_id = extract_patient_scan_id(patient_folder.name)\n",
    "                if patient_id is None or scan_id is None:\n",
    "                    print(f\"Skipping folder: {patient_folder.name}\")\n",
    "                    continue\n",
    "                \n",
    "                segmentation_file = None\n",
    "                for image_file in patient_folder.glob('*.nii.gz'):\n",
    "                    sequence = extract_sequence(image_file.name)\n",
    "                    if sequence == 'seg':\n",
    "                        segmentation_file = image_file\n",
    "                        continue\n",
    "                    sequence_label = sequence_mapping.get(sequence.lower(), 0)\n",
    "                    patient_data[patient_id].append((patient_id, scan_id, sequence, sequence_label, image_file, segmentation_file))\n",
    "        \n",
    "        # Split patients into train, validation, and test sets\n",
    "        patient_ids = list(patient_data.keys())\n",
    "        random.shuffle(patient_ids)\n",
    "        total_patients = len(patient_ids)\n",
    "        train_split = int(total_patients * 0.8)\n",
    "        val_split = int(total_patients * 0.9)\n",
    "        \n",
    "        train_patients = patient_ids[:train_split]\n",
    "        val_patients = patient_ids[train_split:val_split]\n",
    "        test_patients = patient_ids[val_split:]\n",
    "        \n",
    "        print(f\"Total patients: {total_patients}\")\n",
    "        print(f\"Training patients: {len(train_patients)}\")\n",
    "        print(f\"Validation patients: {len(val_patients)}\")\n",
    "        print(f\"Testing patients: {len(test_patients)}\")\n",
    "        \n",
    "        # Process and write data\n",
    "        for subset, patients, writer, images_subset_path, labels_subset_path in [\n",
    "            ('training', train_patients, train_writer, train_images_out, train_labels_out),\n",
    "            ('validation', val_patients, val_writer, val_images_out, val_labels_out),\n",
    "            ('testing', test_patients, test_writer, test_images_out, test_labels_out)\n",
    "        ]:\n",
    "            for patient_id in patients:\n",
    "                for patient_id, scan_id, sequence, sequence_label, image_file, segmentation_file in patient_data[patient_id]:\n",
    "                    # Copy image file\n",
    "                    image_output_file = images_subset_path / f\"{patient_id}-{scan_id}-{sequence}.nii.gz\"\n",
    "                    image_output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    shutil.copyfile(image_file, image_output_file)\n",
    "                    \n",
    "                    # Copy segmentation file if it exists\n",
    "                    has_segmentation = segmentation_file is not None\n",
    "                    if has_segmentation:\n",
    "                        seg_output_file = labels_subset_path / f\"{patient_id}-{scan_id}-seg.nii.gz\"\n",
    "                        seg_output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        shutil.copyfile(segmentation_file, seg_output_file)\n",
    "                    \n",
    "                    row_data = [patient_id, scan_id, sequence, sequence_label, dataset, has_segmentation]\n",
    "                    main_writer.writerow(row_data)\n",
    "                    writer.writerow(row_data)\n",
    "\n",
    "process_dataset(input_path, output_path, output_file, training_csv_file, validation_csv_file, test_csv_file, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
